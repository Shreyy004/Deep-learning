{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df634383-fece-4274-b790-185b3e499d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.9.3-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (2.1.0)\n",
      "Collecting array-record (from tensorflow-datasets)\n",
      "  Downloading array_record-0.4.1-py39-none-any.whl.metadata (503 bytes)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (8.1.7)\n",
      "Collecting dm-tree (from tensorflow-datasets)\n",
      "  Downloading dm_tree-0.1.8-cp39-cp39-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting etils>=0.9.0 (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets)\n",
      "  Downloading etils-1.5.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (1.26.4)\n",
      "Collecting promise (from tensorflow-datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (3.20.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (6.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (2.32.3)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
      "  Downloading tensorflow_metadata-1.17.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (2.4.0)\n",
      "Requirement already satisfied: toml in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (4.66.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-datasets) (1.16.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (2024.6.1)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (6.4.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (4.12.2)\n",
      "Requirement already satisfied: zipp in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (3.19.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->tensorflow-datasets) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Collecting protobuf>=3.20 (from tensorflow-datasets)\n",
      "  Downloading protobuf-4.21.12-cp39-cp39-win_amd64.whl.metadata (541 bytes)\n",
      "Downloading tensorflow_datasets-4.9.3-py3-none-any.whl (5.0 MB)\n",
      "   ---------------------------------------- 0.0/5.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/5.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.5/5.0 MB 1.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.0/5.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.3/5.0 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.3/5.0 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 1.8/5.0 MB 1.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.1/5.0 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.9/5.0 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 3.4/5.0 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.5/5.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.0/5.0 MB 2.3 MB/s eta 0:00:00\n",
      "Downloading etils-1.5.2-py3-none-any.whl (140 kB)\n",
      "Downloading array_record-0.4.1-py39-none-any.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 1.0/3.0 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.6/3.0 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.4/3.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 4.0 MB/s eta 0:00:00\n",
      "Downloading dm_tree-0.1.8-cp39-cp39-win_amd64.whl (101 kB)\n",
      "Downloading tensorflow_metadata-1.17.0-py3-none-any.whl (29 kB)\n",
      "Downloading protobuf-4.21.12-cp39-cp39-win_amd64.whl (527 kB)\n",
      "   ---------------------------------------- 0.0/527.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 527.0/527.0 kB 3.4 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21551 sha256=944672b231ec8261bb3a96ecad67594b048ef3aa4ed6b5ed92620efe5cb73535\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\e1\\e8\\83\\ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, protobuf, promise, etils, tensorflow-metadata, array-record, tensorflow-datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.2\n",
      "    Uninstalling protobuf-3.20.2:\n",
      "      Successfully uninstalled protobuf-3.20.2\n",
      "Successfully installed array-record-0.4.1 dm-tree-0.1.8 etils-1.5.2 promise-2.3 protobuf-4.21.12 tensorflow-datasets-4.9.3 tensorflow-metadata-1.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\google\\~rotobuf'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.71.0rc2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.21.12 which is incompatible.\n",
      "mediapipe 0.10.20 requires protobuf<5,>=4.25.3, but you have protobuf 4.21.12 which is incompatible.\n",
      "onnxconverter-common 1.14.0 requires protobuf==3.20.2, but you have protobuf 4.21.12 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d16d6b1-ac51-4214-81d5-ea5b950ed01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3076b-19f8-4511-859d-776a6765afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load IMDb dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(dataset[\"train\"])  # Use \"test\" for test data\n",
    "\n",
    "# Print first 10 rows\n",
    "print(\"Dataset Loaded Successfully!\")\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bbcb06-ada8-478d-80a1-2dbee0415f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit vocabulary size\n",
    "\n",
    "# Apply TF-IDF transformation\n",
    "X = vectorizer.fit_transform(df[\"text\"]).toarray()  # Transform text to numerical values\n",
    "y = df[\"label\"].values  # Labels (0 = Negative, 1 = Positive)\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df = pd.DataFrame(X, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print first 10 rows of transformed data\n",
    "print(\"\\nFirst 10 Rows (After TF-IDF Transformation):\")\n",
    "print(tfidf_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0d964-d291-4334-a3fd-a4b780353710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sigmoid function for reconstruction\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Split dataset into train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "hidden_units = [64, 128, 256]  \n",
    "best_rbm = None\n",
    "best_loss = float(\"inf\")\n",
    "rbm_losses = {}\n",
    "\n",
    "# Try different hidden units\n",
    "for units in hidden_units:\n",
    "    print(f\"\\nTraining RBM with {units} hidden units...\")\n",
    "    \n",
    "    rbm = BernoulliRBM(n_components=units, learning_rate=0.01, n_iter=5, verbose=True, random_state=42)  # Reduced iterations\n",
    "    \n",
    "    # Train RBM\n",
    "    rbm.fit(X_train)\n",
    "    \n",
    "    # Transform input through hidden layer\n",
    "    hidden_features = rbm.transform(X_train)\n",
    "    \n",
    "    # Reconstruct input using visible probabilities (sigmoid activation)\n",
    "    reconstructed_X = sigmoid(np.dot(hidden_features, rbm.components_))\n",
    "    \n",
    "    # Compute mean squared error (MSE) as reconstruction loss\n",
    "    loss = np.mean(np.square(X_train - reconstructed_X))  \n",
    "    \n",
    "    rbm_losses[units] = loss\n",
    "    \n",
    "    # Keep track of the best RBM\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_rbm = rbm\n",
    "\n",
    "print(\"\\nHyperparameter tuning completed!\")\n",
    "print(f\"Best RBM has {best_rbm.n_components} hidden units with loss: {best_loss:.4f}\")\n",
    "\n",
    "# Plot loss for different hyperparameters\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(rbm_losses.keys(), rbm_losses.values(), marker='o', linestyle='--', color='r')\n",
    "plt.xlabel(\"Number of Hidden Units\")\n",
    "plt.ylabel(\"Reconstruction Loss\")\n",
    "plt.title(\"RBM Hyperparameter Tuning: Hidden Units vs. Loss\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd559035-2fa2-444b-a889-427a718ae19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_rbm.n_iter = 50  \n",
    "loss_curve = []\n",
    "for epoch in range(1, 51):  \n",
    "    best_rbm.fit(X_train)  \n",
    "    reconstructed_X = best_rbm.transform(X_train)  \n",
    "    loss = np.mean(np.square(X_train - best_rbm.inverse_transform(reconstructed_X)))  # MSE\n",
    "    loss_curve.append(loss)\n",
    "    print(f\"Epoch {epoch}/50 - Loss: {loss:.4f}\")\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 51), loss_curve, marker='o', linestyle='-', color='b', label=\"Reconstruction Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"RBM Training Loss Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b3de6e-6b66-4dcd-9d77-377b486a1b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e35eef-108f-44b2-a03d-4a403fd9a41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded Successfully!\n",
      "                                                text  label\n",
      "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
      "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
      "2  If only to avoid making this type of film in t...      0\n",
      "3  This film was probably inspired by Godard's Ma...      0\n",
      "4  Oh, brother...after hearing about this ridicul...      0\n",
      "5  I would put this at the top of my list of film...      0\n",
      "6  Whoever wrote the screenplay for this movie ob...      0\n",
      "7  When I first saw a glimpse of this movie, I qu...      0\n",
      "8  Who are these \"They\"- the actors? the filmmake...      0\n",
      "9  This is said to be a personal film for Peter B...      0\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load IMDb dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "df = pd.DataFrame(dataset[\"train\"])  \n",
    "\n",
    "# Print first 10 rows\n",
    "print(\"Dataset Loaded Successfully!\")\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b02153-8d71-41e3-8df4-06667a305ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 Rows (After TF-IDF Transformation):\n",
      "    00  000        10  100   11   12   13  13th   14   15  ...     young  \\\n",
      "0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.043502   \n",
      "1  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.000000   \n",
      "2  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.000000   \n",
      "3  0.0  0.0  0.084063  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.000000   \n",
      "4  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.047706   \n",
      "5  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.000000   \n",
      "6  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.000000   \n",
      "7  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.000000   \n",
      "8  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.000000   \n",
      "9  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.000000   \n",
      "\n",
      "   younger      your  yourself  youth  zero  zizek  zombie  zombies  zone  \n",
      "0      0.0  0.000000       0.0    0.0   0.0    0.0     0.0      0.0   0.0  \n",
      "1      0.0  0.000000       0.0    0.0   0.0    0.0     0.0      0.0   0.0  \n",
      "2      0.0  0.000000       0.0    0.0   0.0    0.0     0.0      0.0   0.0  \n",
      "3      0.0  0.000000       0.0    0.0   0.0    0.0     0.0      0.0   0.0  \n",
      "4      0.0  0.000000       0.0    0.0   0.0    0.0     0.0      0.0   0.0  \n",
      "5      0.0  0.000000       0.0    0.0   0.0    0.0     0.0      0.0   0.0  \n",
      "6      0.0  0.000000       0.0    0.0   0.0    0.0     0.0      0.0   0.0  \n",
      "7      0.0  0.000000       0.0    0.0   0.0    0.0     0.0      0.0   0.0  \n",
      "8      0.0  0.000000       0.0    0.0   0.0    0.0     0.0      0.0   0.0  \n",
      "9      0.0  0.104059       0.0    0.0   0.0    0.0     0.0      0.0   0.0  \n",
      "\n",
      "[10 rows x 5000 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit vocabulary size\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"text\"]).toarray()  # Transform text to numerical values\n",
    "y = df[\"label\"].values  # Labels (0 = Negative, 1 = Positive)\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df = pd.DataFrame(X, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print first 10 rows of transformed data\n",
    "print(\"\\nFirst 10 Rows (After TF-IDF Transformation):\")\n",
    "print(tfidf_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c677e-1828-483c-a6b7-841f5810a1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RBM with 64 hidden units...\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -19.59, time = 34.71s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -11.15, time = 35.90s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -10.31, time = 35.19s\n",
      "\n",
      "Training RBM with 128 hidden units...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sigmoid function for reconstruction\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Split dataset into train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "hidden_units = [64, 128, 256]  \n",
    "best_rbm = None\n",
    "best_loss = float(\"inf\")\n",
    "rbm_losses = {}\n",
    "\n",
    "# Try different hidden units\n",
    "for units in hidden_units:\n",
    "    print(f\"\\nTraining RBM with {units} hidden units...\")\n",
    "    \n",
    "    rbm = BernoulliRBM(n_components=units, learning_rate=0.01, n_iter=3, verbose=True, random_state=42)  # Reduced iterations\n",
    "    \n",
    "    # Train RBM\n",
    "    rbm.fit(X_train)\n",
    "    \n",
    "    # Transform input through hidden layer\n",
    "    hidden_features = rbm.transform(X_train)\n",
    "    \n",
    "    # Reconstruct input using visible probabilities (sigmoid activation)\n",
    "    reconstructed_X = sigmoid(np.dot(hidden_features, rbm.components_))\n",
    "    \n",
    "    # Compute mean squared error (MSE) as reconstruction loss\n",
    "    loss = np.mean(np.square(X_train - reconstructed_X))  \n",
    "    \n",
    "    rbm_losses[units] = loss\n",
    "    \n",
    "    # Keep track of the best RBM\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_rbm = rbm\n",
    "\n",
    "print(\"\\nHyperparameter tuning completed!\")\n",
    "print(f\"Best RBM has {best_rbm.n_components} hidden units with loss: {best_loss:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(rbm_losses.keys(), rbm_losses.values(), marker='o', linestyle='--', color='r')\n",
    "plt.xlabel(\"Number of Hidden Units\")\n",
    "plt.ylabel(\"Reconstruction Loss\")\n",
    "plt.title(\"RBM Hyperparameter Tuning: Hidden Units vs. Loss\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11ebb90-0bd7-4fdb-8158-020b8188abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_rbm.n_iter = 1  \n",
    "epochs = 25 \n",
    "loss_curve = []\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    best_rbm.fit(X_train)  \n",
    "    hidden_features = best_rbm.transform(X_train) \n",
    "    reconstructed_X = sigmoid(np.dot(hidden_features, best_rbm.components_))  # Reconstruct input\n",
    "    loss = np.mean(np.square(X_train - reconstructed_X))  # Compute MSE loss\n",
    "    \n",
    "    loss_curve.append(loss)\n",
    "    print(f\"Epoch {epoch}/{epochs} - Loss: {loss:.4f}\")\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, epochs + 1), loss_curve, marker='o', linestyle='-', color='b', label=\"Reconstruction Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"RBM Training Loss Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6fcb6-7db1-443d-9168-9cd71977d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Transform data using RBM\n",
    "X_train_rbm = best_rbm.transform(X_train)  # Extract features\n",
    "X_test_rbm = best_rbm.transform(X_test)    # Extract features from test set\n",
    "\n",
    "# Convert labels to tensors\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)  # Encode labels\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_rbm, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_rbm, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_enc, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_enc, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"RBM feature extraction complete! New feature shape: {X_train_rbm.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccde1d-0534-48a5-b394-b52890a0645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train_rbm.shape[1]  # Number of RBM features\n",
    "num_classes = len(np.unique(y_train_enc))  # Number of classes\n",
    "model = CNN_Model(input_size, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"CNN model initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa30bc23-7039-49a0-b757-b00c7eb7d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20  # Reduce training time\n",
    "train_loss_curve = []\n",
    "train_acc_curve = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    train_loss_curve.append(avg_loss)\n",
    "    train_acc_curve.append(accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Plot Loss & Accuracy Curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs+1), train_loss_curve, marker='o', linestyle='-', color='b', label=\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"CNN Training Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs+1), train_acc_curve, marker='o', linestyle='-', color='g', label=\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"CNN Training Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dcbe75-b112-4da5-adc1-6da0858ca85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Run model on test set\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        y_pred.extend(predicted.numpy())\n",
    "        y_true.extend(labels.numpy())\n",
    "\n",
    "print(\"Predictions completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92754ebd-22d2-417a-9b20-b0a9e2b66b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"✅ Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"✅ Precision: {precision:.4f}\")\n",
    "print(f\"✅ Recall:    {recall:.4f}\")\n",
    "print(f\"✅ F1 Score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca5b95-219d-4e5a-9f29-e79eda1123f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c5601-bcee-4b2b-9f2e-605ae57c472b",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f54688-8b83-42e2-95d2-a6f344d39b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA (keep 95% variance)\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print(f\"PCA reduced dimensions from {X_train.shape[1]} to {X_train_pca.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8166c1-baf0-42ac-8c18-d646e2a00c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PCA features & labels to tensors\n",
    "X_train_tensor_pca = torch.tensor(X_train_pca, dtype=torch.float32)\n",
    "X_test_tensor_pca = torch.tensor(X_test_pca, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_enc, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_enc, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset_pca = TensorDataset(X_train_tensor_pca, y_train_tensor)\n",
    "test_dataset_pca = TensorDataset(X_test_tensor_pca, y_test_tensor)\n",
    "\n",
    "train_loader_pca = DataLoader(train_dataset_pca, batch_size=32, shuffle=True)\n",
    "test_loader_pca = DataLoader(test_dataset_pca, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed7575-43e9-4c6e-aafa-0c852f1644af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the same CNN model\n",
    "model_pca = CNN_Model(X_train_pca.shape[1], num_classes)\n",
    "\n",
    "# Define loss function & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_pca = optim.Adam(model_pca.parameters(), lr=0.001)\n",
    "\n",
    "print(\"CNN model initialized for PCA features!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef537d-7239-4e8d-8b2b-7dda77b440e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20  \n",
    "train_loss_curve_pca = []\n",
    "train_acc_curve_pca = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_pca.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader_pca:\n",
    "        optimizer_pca.zero_grad()\n",
    "        outputs = model_pca(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_pca.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader_pca)\n",
    "    accuracy = 100 * correct / total\n",
    "    train_loss_curve_pca.append(avg_loss)\n",
    "    train_acc_curve_pca.append(accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Plot Loss & Accuracy Curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs+1), train_loss_curve_pca, marker='o', linestyle='-', color='b', label=\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"PCA+CNN Training Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs+1), train_acc_curve_pca, marker='o', linestyle='-', color='g', label=\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"PCA+CNN Training Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41abee-2481-4f55-b974-59b70f232527",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_pca = []\n",
    "y_true_pca = []\n",
    "\n",
    "# Run model on test set\n",
    "model_pca.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader_pca:\n",
    "        outputs = model_pca(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        y_pred_pca.extend(predicted.numpy())\n",
    "        y_true_pca.extend(labels.numpy())\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy_pca = accuracy_score(y_true_pca, y_pred_pca)\n",
    "precision_pca = precision_score(y_true_pca, y_pred_pca, average=\"weighted\")\n",
    "recall_pca = recall_score(y_true_pca, y_pred_pca, average=\"weighted\")\n",
    "f1_pca = f1_score(y_true_pca, y_pred_pca, average=\"weighted\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n📊 PCA+CNN Classification Report:\")\n",
    "print(classification_report(y_true_pca, y_pred_pca))\n",
    "\n",
    "# Print metrics\n",
    "print(f\"✅ Accuracy:  {accuracy_pca:.4f}\")\n",
    "print(f\"✅ Precision: {precision_pca:.4f}\")\n",
    "print(f\"✅ Recall:    {recall_pca:.4f}\")\n",
    "print(f\"✅ F1 Score:  {f1_pca:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5058c6b9-ace8-48f0-b826-8f0969d2f900",
   "metadata": {},
   "source": [
    "# COMPARISON OF PCA+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b07e4a-2480-48cd-b90c-d65a1803c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison table\n",
    "print(\"\\n🔹 Performance Comparison: RBM+CNN vs. PCA+CNN\")\n",
    "print(f\"{'Metric':<15}{'RBM+CNN':<10}{'PCA+CNN':<10}\")\n",
    "print(f\"{'-'*35}\")\n",
    "print(f\"{'Accuracy':<15}{accuracy:.4f}    {accuracy_pca:.4f}\")\n",
    "print(f\"{'Precision':<15}{precision:.4f}    {precision_pca:.4f}\")\n",
    "print(f\"{'Recall':<15}{recall:.4f}    {recall_pca:.4f}\")\n",
    "print(f\"{'F1 Score':<15}{f1:.4f}    {f1_pca:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f001d98-26f2-42ec-a576-41f5a4f0e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define metrics & values\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
    "rbm_values = [accuracy, precision, recall, f1]\n",
    "pca_values = [accuracy_pca, precision_pca, recall_pca, f1_pca]\n",
    "\n",
    "# Set bar width\n",
    "bar_width = 0.3  \n",
    "index = np.arange(len(metrics))\n",
    "\n",
    "# Plot bars\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(index, rbm_values, bar_width, label=\"RBM+CNN\", color=\"blue\")\n",
    "plt.bar(index + bar_width, pca_values, bar_width, label=\"PCA+CNN\", color=\"green\")\n",
    "\n",
    "# Labels & Title\n",
    "plt.xlabel(\"Evaluation Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"RBM+CNN vs PCA+CNN Performance Comparison\")\n",
    "plt.xticks(index + bar_width / 2, metrics)  # Set x-ticks at center\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
